{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BME-336546-C08-Unsupervised learning - Dimensionality reduction (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "   * ID number.\n",
    "   * Diagnosis (M = malignant, B = benign).\n",
    "   \n",
    "Ten real-valued features are computed for each cell nucleus:\n",
    "\n",
    "   * Radius (mean of distances from center to points on the perimeter) $[mm]$.\n",
    "   * Texture (standard deviation of gray-scale values) $[N.U]$.\n",
    "   * Perimeter $[mm]$.\n",
    "   * Area $[mm^2]$.\n",
    "   * Smoothness (local variation in radius lengths) $[mm]$.\n",
    "   * Compactness (perimeter² / area — 1.0) $[N.U]$.\n",
    "   * Concavity (severity of concave portions of the contour) $[N.U]$.\n",
    "   * Concave points (number of concave portions of the contour) $[N.U]$.\n",
    "   * Symmetry $[N.U]$\n",
    "   * Fractal dimension (“coastline approximation” — 1) $[N.U]$.\n",
    "    \n",
    "The mean, standard error and “worst” or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 of the table is the Mean Radius, 13 is the Radius SE and 23 is the Worst Radius."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main ML topic:\n",
    "This tutorial will also deal with unsupervised learning. As we mentioned previously, we can find multiple uses in this field such as:\n",
    "* Dimensionality reduction.\n",
    "* Discovery of the data's structure (geometry of the data).\n",
    "* Feature selection.\n",
    "* Clustering.\n",
    "* Visualization of high-dimensional data.\n",
    "* Blind source separation (BSS).\n",
    "* Data denoisng.\n",
    "* Simulating data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory reminders\n",
    "Many times, we encounter high-dimensional data that most of the \"information\" that it carries actually lies in a much lower dimensional space. For example, gray level images of the size $MXN$ have dimensionallity of $MN$ which can be very large even for relatively small images. However, if you take the image grid and randomize a number for every pixel, the chances to produce a meaningful image are almost 0. This implies that natural images of this size for instance, may \"live\" in another lower dimensional space. Just for vizualization, here is an example of $3D$ data that is actually embedded in a $2D$ space. \n",
    "\n",
    "<center><img src=\"images/2din3.png\" width=\"550\"><center>\n",
    "<!-- ![image alt ><](/2din3.jpg) -->\n",
    "<!-- <img src= <center>![2din3.png]</center> -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curse of dimensionality\n",
    "High dimensional datasets are at risk of being very sparse: most training instances are likely to be far away from each other.\n",
    "Of course, this also means that a new instance will likely be far away from any training instance, making predictions much less reliable than in lower dimensions, since they will be based on much larger extrapolations.\n",
    "In short, the more dimensions the training set has, the greater the risk of overfitting the model.\n",
    "Therefore a dimension reduction method should sometime be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principle component analysis (PCA) is a linear unsupervised learning method mostly used for dimensionlaity reduction. It tries to find a linear surface (1D line, a plane or a hyper plane) where the data can be projected to and still maintain as much information as possible. Assuming $n$ samples of dimension $m$ that are stacked in the matrix $X \\in \\mathbb{R}^{mxn} $. Our goal is to acheive a matrix $Z \\in \\mathbb{R}^{kxn}$ where $k<<m$. In PCA, information is considered to be preserved where the variance of the projected data is maximal. In other words we are looking for a new **orthonormal** coordinates system composed of the columns of the matrix $U$. $$ \\begin{equation}\n",
    "U=\\begin{pmatrix}\n",
    "u_1| & u_2| & \\dots|& u_k\n",
    "\\end{pmatrix} \\in \\mathbb{R}^{mxk}\n",
    "\\label{eq:U} \\tag{1}\n",
    "\\end{equation} $$\n",
    "Once we project our data on to the new subspace we get: $$ \\begin{equation}\n",
    "Z=U^TX \\in \\mathbb{R}^{kxn}\n",
    "\\label{eq:Z} \\tag{2}\n",
    "\\end{equation} $$\n",
    "The sampled covarince of the projected data is calculated as follows:$$ \\begin{equation}\n",
    "Cov(Z) = ZZ^T = U^TXX^TU = U^TCU\\\\\n",
    "Var(Z) = trace(Cov(Z))\n",
    "\\label{eq:var_z} \\tag{3}\n",
    "\\end{equation} $$\n",
    "where $C$ is the sampled covariance matrix of **centered** $X$ and thus, $X$ should be centered prior to all other calculations. Practically it should also be scaled. Now, we can set a maximization problem of $U$ subject to a constraint: $$ \\begin{equation}\n",
    "\\max_{U}\\{trace(U^TCU)\\} \\space s.t. \\space U^TU=I\n",
    "\\label{eq:optimization} \\tag{4}\n",
    "\\end{equation} $$\n",
    "It was shown in the lecture that the solution of this problem is the matrix of the first $k^{th}$ eigenvectors of $C$, ordered by their descending corresponding $k$ eigenvalues. The eigenvalues are the variences of the projected data along each corresponding axis. It is also equivalent to minimizing the $L_2$ norm of the sum of the orthogonal distances of the data points from the $i^{th}$ axis $(u_i)$ simply due to Pythagoras' theorem as shown heuristicaly in the following gifs. The varince is simply the sum of the distances of the green X's (the projections) on the rotating axis in the left gif.\n",
    "\n",
    "<left><img src=\"images/min_max.gif\" width=\"350\"><left>\n",
    "<right><img src=\"images/pca.gif\" width=\"450\"><right>\n",
    "    \n",
    "Notice that it is **not** the same as linear regression because in linear regression we try to minimize the $L_2$ norm of the residuals which are the \"distances\" that are orthogonal to the original axis and not to the new one ($u_i$) as shown below:\n",
    "<center><img src=\"images/PCA_vs_linear_regression.png\" width=\"400\"><center>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it can also be thought as finding the principle axes of a fitted elipsoid. \n",
    "\n",
    "<center><img src=\"images/elipsoid.png\" width=\"350\"><center>\n",
    "\n",
    "Now if we would like to have an approximation of $X$ by using the low dimensional space, all we have to do is simply calculate the linear combination of the $U$ basis. Fortunetly, Our basis is orthonormal, and thus the corresponding coefficient is simply the projections. $$ \\begin{equation}\n",
    "\\tilde{X}=UZ\\in \\mathbb{R}^{mxn}\\\\\n",
    "\\label{eq:approx} \\tag{5}\n",
    "\\end{equation} $$\n",
    "The higher dimension you choose $(k)$, the more \"information\" or \"energy\" you gain but  less dimensionality reduction acheived. $$ \\begin{equation}\n",
    "\\parallel \\tilde{X}-X\\parallel^2_2 \\space \\overset{k \\rightarrow m}{\\longrightarrow} \\space \\bar{0}\n",
    "\\label{eq:approx energy} \\tag{6}\n",
    "\\end{equation} $$\n",
    "\n",
    "\n",
    "One way of choosing  k is to calculate the ratio of the preserved energy as follows: $$ \\begin{equation}\n",
    "r=\\frac{\\lambda_1+\\lambda_2+\\dots +\\lambda_k}{\\lambda_1+\\lambda_2+\\dots +\\lambda_k +\\dots + \\lambda_m}\n",
    "\\label{eq:energy} \\tag{7}\n",
    "\\end{equation} $$\n",
    "The $k$ that brings that ratio to be larger than 0.95 is considered to be well preserving and hopefully it still satisfies $k<<m$.\n",
    "\n",
    "\n",
    "PCA has also its' nonlinear form which is similar to what we have seen in SVM. So we define a nonlinear function $\\Phi(x)$ and our covariance matrix is now $C=\\Phi(X)\\Phi^T(X)$. Centering should be applied here as well and the kernel trick is also applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.style.use(['ggplot']) \n",
    "%matplotlib inline\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from IPython.display import display, clear_output\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "# from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "# import scipy.io as sio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by loading the data that we had in our fourth tutorial directly from `sklearn.datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loading have some methods and attributes that are useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = pd.DataFrame(data['data'],columns=data['feature_names'])\n",
    "df_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with choosing three features **without scaling them**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_name =['mean radius', 'mean concavity', 'mean symmetry']\n",
    "X3d = df_feat[feat_name]\n",
    "y = data['target']\n",
    "X3d = X3d.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X3d, y, test_size=0.1, stratify=y, random_state=336546)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce the dimensionaliy of both the training set and the testing set to 2 using PCA. Set `whiten=True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notice: the eigenvectors in sklearn are row vectors and thus $UU^T=I$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C1\n",
    "n_components = 2\n",
    "#---------------------------Implement your code here:------------------------\n",
    "\n",
    "#----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "def plt_3d_orig(X,y,feat_name):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    ax.scatter(X[y==0, 0], X[y==0, 1], X[y==0, 2], color='b')# c=X[y==0, 2], cmap='Blues', label='B');\n",
    "    ax.scatter(X[y==1, 0], X[y==1, 1], X[y==1, 2], color='r')#c=X[y==1, 2], cmap='Reds', label='M')\n",
    "    \n",
    "    ax.set_xlabel(feat_name[0])\n",
    "    ax.set_ylabel(feat_name[1])\n",
    "    ax.set_zlabel(feat_name[2])\n",
    "    ax.legend(('B','M'))\n",
    "    ax.set_title('Original Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the three selected variables of the testing set in 3D space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_3d_orig(X_test,y_test,feat_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the same data reduced into 2D space using PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_2d_pca(X_pca,y):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(111, aspect='equal')\n",
    "    ax.scatter(X_pca[y==0, 0], X_pca[y==0, 1], color='b')\n",
    "    ax.scatter(X_pca[y==1, 0], X_pca[y==1, 1], color='r')\n",
    "    ax.legend(('B','M'))\n",
    "    ax.plot([0], [0], \"ko\")\n",
    "    ax.arrow(0, 0, 0, 1, head_width=0.05, length_includes_head=True, head_length=0.1, fc='k', ec='k')\n",
    "    ax.arrow(0, 0, 1, 0, head_width=0.05, length_includes_head=True, head_length=0.1, fc='k', ec='k')\n",
    "    ax.set_xlabel('$U_1$')\n",
    "    ax.set_ylabel('$U_2$')\n",
    "    ax.set_title('2D PCA')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_2d_pca(Z_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "<center><img src=\"outputs/1.PNG\" width=\"480\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name your reduced test set as `X2D` and use the `inverse_transform` method to reconstruct the data into 3D space. Visulaize the results using `plt_3d_orig_vs_inv_pca` and evaluate the loss of information using `np.allcose()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C2\n",
    "#---------------------------Implement your code here:------------------------\n",
    "\n",
    "#---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import FancyArrowPatch\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "class Arrow3D(FancyArrowPatch):\n",
    "    def __init__(self, xs, ys, zs, *args, **kwargs):\n",
    "        FancyArrowPatch.__init__(self, (0,0), (0,0), *args, **kwargs)\n",
    "        self._verts3d = xs, ys, zs\n",
    "\n",
    "    def draw(self, renderer):\n",
    "        xs3d, ys3d, zs3d = self._verts3d\n",
    "        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M)\n",
    "        self.set_positions((xs[0],ys[0]),(xs[1],ys[1]))\n",
    "        FancyArrowPatch.draw(self, renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_3d_orig_vs_inv_pca(X,X2D,X3D_inv,pca,obj):\n",
    "    x1s = np.linspace(X2D[:,0].min(), X2D[:,0].max(), 10)\n",
    "    x2s = np.linspace(X2D[:,1].min(), X2D[:,1].max(), 10)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "\n",
    "    U = pca.components_\n",
    "    R = U.T.dot(U)\n",
    "    z = (R[0, 2] * x1 + R[1, 2] * x2) / (1 - R[2, 2])\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot_surface(x1, x2, z, alpha=0.2, color=\"k\")\n",
    "    np.linalg.norm(U, axis=0)\n",
    "    ax.add_artist(obj([0, U[0, 0]],[0, U[0, 1]],[0, U[0, 2]], mutation_scale=15, lw=1, arrowstyle=\"-|>\", color=\"g\"))\n",
    "    ax.add_artist(obj([0, U[1, 0]],[0, U[1, 1]],[0, U[1, 2]], mutation_scale=15, lw=1, arrowstyle=\"-|>\", color=\"g\"))\n",
    "    ax.plot([0], [0], [0], \"g.\")\n",
    "    ax.plot(X3D_inv[:, 0], X3D_inv[:, 1], X3D_inv[:, 2], \"k.\")\n",
    "    ax.plot(X[:, 0], X[:, 1], X[:, 2], \"b.\")\n",
    "    ax.set_xlabel(\"$x_1$\", fontsize=18)\n",
    "    ax.set_ylabel(\"$x_2$\", fontsize=18)\n",
    "    ax.set_zlabel(\"$x_3$\", fontsize=18)\n",
    "    ax.view_init(30, 20)\n",
    "    plt.legend(('Eig. Basis','Rec.','Orig'))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_3d_orig_vs_inv_pca(X_test,X2D,X3D_inv,pca,Arrow3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "<center><img src=\"outputs/2.PNG\" width=\"480\"><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C3\n",
    "#---------------------------Implement your code here:------------------------\n",
    "\n",
    "#----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Question:***</span> *Which of the equations above have you just implemented using `inverse_transform`? What do we need to do in order to improve our results?*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you answered the second question, implement your answer and repeat all of the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C4\n",
    "#---------------------------Implement your code here:------------------------\n",
    "\n",
    "#----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_3d_orig(X_test,y_test,feat_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_2d_pca(Z_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "<center><img src=\"outputs/3.PNG\" width=\"480\"><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C5\n",
    "#---------------------------Implement your code here:------------------------\n",
    "\n",
    "#----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_3d_orig_vs_inv_pca(X_test,X2D,X3D_inv,pca,Arrow3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "<center><img src=\"outputs/4.PNG\" width=\"480\"><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C6\n",
    "#---------------------------Implement your code here:------------------------\n",
    "\n",
    "#----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said, we can reduce our data dimensionality as much as we want. Our original data have 30 featurs. Let's scale the data and reduce it to both 2D and 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_feat.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a PCA\n",
    "n_components = X.shape[1]\n",
    "pca = PCA(n_components=n_components, whiten=True)\n",
    "\n",
    "# apply PCA transformation\n",
    "Z_train = pca.fit_transform(X_train)\n",
    "Z_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_2d_pca(Z_test[:,0:2],y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "<center><img src=\"outputs/5.PNG\" width=\"480\"><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_3d_pca(X_pca,y):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(X_pca[y==0, 0], X_pca[y==0, 1], X_pca[y==0, 2], color='b');\n",
    "    ax.scatter(X_pca[y==1, 0], X_pca[y==1, 1], X_pca[y==1, 2], color='r')\n",
    "    # sns.scatterplot(X_train[:, 0], X_train[:, 1], X_train[:, 2], hue=y_train)\n",
    "    ax.legend(('B','M'))\n",
    "    ax.set_title('3D PCA')\n",
    "    ax.set_xlabel('$U_1$')\n",
    "    ax.set_ylabel('$U_2$')\n",
    "    ax.set_zlabel('$U_3$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_3d_pca(Z_test[:,0:3],y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "<center><img src=\"outputs/6.PNG\" width=\"480\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<span style=\"color:red\">***Question:***</span> *Let's say that we to conserve 95% of the \"information\" that we have in our data. How do we do that?*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a bar plot (using `plt.bar()`) that shows the value of $\\lambda$ for every eigenvectors. The eigenvalues are computed within the `PCA` attribute named `explained_variance_`. Calculate the accumelated \"energy\" conserved for every eignevector using equation (7) and find the number of eigenvectors needed in order to conserve 95% of the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C7\n",
    "#---------------------------Implement your code here:------------------------\n",
    "\n",
    "#----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "<center><img src=\"outputs/7.PNG\" width=\"480\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, `scikit-learn` can do this for us automatically once we choose to set a fraction in `n_components` argument and use `explained_variance_ratio_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "Z_train = pca.fit_transform(X_train)\n",
    "Z_test = pca.transform(X_test)\n",
    "print('{:.2f}% of data information was conserved.'.format(100*np.sum(pca.explained_variance_ratio_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use PCA as a perprocessing of the classification task. This should help us prevent overfitting and run models faster. Use `PipeLine` and create two pipes: one without PCA and one with PCA (95%). Use the *logistic regression* model and notice the scaling. Compare their scores on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C8\n",
    "X = df_feat.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "log_reg = LogisticRegression()\n",
    "#---------------------------Implement your code here:------------------------\n",
    "\n",
    "#------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "<center><img src=\"outputs/8.PNG\" width=\"480\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<span style=\"color:red\">***Question:***</span> *Did you get any better results?*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will see an example of how to apply *kernel PCA* with *rbf* kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "k_pca = KernelPCA(n_components=4, kernel='rbf')\n",
    "pipe_k_pca = Pipeline(steps=[('scale', StandardScaler()),('k_pca', k_pca)])\n",
    "pipe_k_pca.fit_transform(X_train)\n",
    "Z_test = pipe_k_pca.transform(X_test)\n",
    "plt_2d_pca(Z_test[:,0:2],y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_3d_pca(Z_test[:,0:3],y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *This tutorial was written by [Moran Davoodi](mailto:morandavoodi@gmail.com) with the assitance of Alon Begin, [Yuval Ben Sason](mailto:yuvalbse@gmail.com) & Kevin Kotzen*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
