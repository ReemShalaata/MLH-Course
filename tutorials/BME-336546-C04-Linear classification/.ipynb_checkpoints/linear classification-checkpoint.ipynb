{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BME-336546-C04-Linear classification (LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medical topic\n",
    "According to the American Cancer Society (ACS), breast cancer is the second most common cancer in American women, after skin cancer. Currently, the average risk of a woman in the United States developing breast cancer sometime during her life is ~13%. This means that around every 1 in 8 women will develop breast cancer. \n",
    "\n",
    "ACS's estimates for breast cancer in the United States for 2020 are: \n",
    "\n",
    "* About 276,480 new cases of invasive breast cancer will be diagnosed in women. \n",
    "* About 48,530 new cases of carcinoma-in-situ (CIS) will be diagnosed (CIS is non-invasive and is the earliest form of breast cancer). \n",
    "* About 42,170 women will die from breast cancer.\n",
    "\n",
    "Breast cancer is a type of cancer that starts in the breast. In general, cancer starts when cells begin to grow out of control. \n",
    "Breast cancer cells usually form a tumor that can often be seen on an x-ray or felt as a lump. Breast cancer occurs almost entirely in women, but men can get breast cancer too.\n",
    "\n",
    "It is important to understand that most breast lumps are benign and not cancerous (malignant). Non-cancerous breast tumors are abnormal growths, but they do not spread outside of the breast. They are not life threatening, but, in some cases, can increase a woman's risk of getting breast cancer. Any breast lump or change needs to be checked by a health care professional to determine if it is benign or malignant and if it might affect your future cancer risk. This is why a correct classification is highly important. \n",
    "\n",
    "Attached an image for comparison:\n",
    "<center><img src=\"images/b.jpg\" width=\"400\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "   * ID number.\n",
    "   * Diagnosis (M = malignant, B = benign).\n",
    "   \n",
    "Ten real-valued features are computed for each cell nucleus:\n",
    "\n",
    "   * Radius (mean of distances from center to points on the perimeter) $[mm]$.\n",
    "   * Texture (standard deviation of gray-scale values) $[N.U]$.\n",
    "   * Perimeter $[mm]$.\n",
    "   * Area $[mm^2]$.\n",
    "   * Smoothness (local variation in radius lengths) $[mm]$.\n",
    "   * Compactness (perimeter² / area — 1.0) $[N.U]$.\n",
    "   * Concavity (severity of concave portions of the contour) $[N.U]$.\n",
    "   * Concave points (number of concave portions of the contour) $[N.U]$.\n",
    "   * Symmetry $[N.U]$\n",
    "   * Fractal dimension (“coastline approximation” — 1) $[N.U]$.\n",
    "    \n",
    "The mean, standard error and “worst” or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 of the table is the Mean Radius, 13 is the Radius SE and 23 is the Worst Radius. [Data credit.](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main ML topic:\n",
    "The main ML topic in this tutorial is ***supervised learning*** and specifically ***binary classification***. Binary classification is one of the most classical tasks in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our mission\n",
    "Classify the lump type using binary classification. Classification is performed on part of the dataset (train-set) and then tested on a hidden set (test-set). In that respect, *'classification'* is actually a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory reminders\n",
    "In binary linear classification tasks we try to find a **linear** surface that separates the two groups (labeled as red and blue, 0 and 1 etc.). Visualizations of a $2D$ and a $3D$ cases are shown below - \n",
    "\n",
    "\n",
    "<left><img src=\"images/2d linear.png\" width=\"280\"><left>\n",
    "<right><img src=\"images/3d linear.gif\" width=\"350\" height=\"50\"><right>\n",
    "\n",
    "Let's assume we have $m$ examples (data points). Each of them has its *feature vector* $x \\in \\mathbb{R}^D$ and a label $y$ that can have a binary value. Fortunately, algebra does not care about dimensionality. As far as the math is concerned, any linear plane in any dimension can be represented by the next compact equation:\n",
    "    \n",
    "$$\\begin{equation}\n",
    "w^{T}(x-x_0)=w^{T}x + b = 0\n",
    "\\label{eq:linear plane} \\tag{1}\n",
    "\\end{equation}\n",
    "$$\n",
    "    \n",
    "    \n",
    "Where $w^{T}$ is the vector that is perpendicular to the plane (and thus tells us about its orientation in space) and $x_0$ is the vector that fixes our plane in space (a point in space) . \n",
    "    \n",
    "Now we will define $z=w^{T}x+b$. For a fixed set of $\\{w^{T},b\\}$, every $x$ vector that will be mapped to $z>0$ will be on one side of the plane, and every $x$ vector that will be mapped to $z<0$ will be on the other side of the plane. This linear hypothesis is also known as ***perceptron***.\n",
    "    \n",
    "Similarly to linear regression, we would like to deal with algebraic entities that can be represented as a matrix multiplication.\\\n",
    "In order to do so, we will use the same notations as before: $\\tilde{x}=\\begin{pmatrix}1\\\\x\\end{pmatrix} \\in \\mathbb{R}^{D+1}$ and $\\tilde{w}=\\begin{pmatrix}b\\\\w\\end{pmatrix} \\in \\mathbb{R}^{D+1}$ so we can write the combined modified equation as follows:  $z={\\tilde{w}^{T}}\\tilde{x}$. Convince yourself that this is equal to the inner product defined in (1). \n",
    "    \n",
    "From now on we will drop the tilde sign and use $z=w^{T}x$ for the modified equation. Our goal is to find (\"**learn**\") $w$ that maps $x$ correctly to $z$ and by that, classify correctly our examples. The classification is done by labeling an output variable $y$ as 1 or 0 accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is convenient to look at our problem from a probabilistic aspect using the *logistic* function for instance:\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\hat{p}=\\begin{pmatrix}\\hat{P}(Y=1|X=x)\\\\\\hat{P}(Y=0|X=x)\\end{pmatrix}=\\begin{pmatrix}\\frac{e^{w^{T}x}}{1+e^{w^{T}x}}\\\\\\frac{1}{1+e^{w^{T}x}}\\end{pmatrix}\n",
    "=\\begin{pmatrix}\\sigma(z)\\\\1-\\sigma(z)\\end{pmatrix}\n",
    "\\label{eq: sigmoid} \\tag{2}\n",
    "\\end{equation}$$\n",
    "\n",
    "Labeling is applied by setting a threshold on *$\\sigma(z)$* (we often use 0.5 as a threshold). \\\n",
    "Now, we would like to compare our *hypothesized* labels to our *true* labels. First, we encode our true labels to *one-hot-vector* so it can be interpreted as probability:\n",
    "\n",
    "$$\\begin{equation}\n",
    "y = 1 \\rightarrow p=\\begin{pmatrix}P(Y=1|X=x)\\\\P(Y=0|X=x)\\end{pmatrix}=\\begin{pmatrix}1\\\\0\\end{pmatrix} \\\\\n",
    "y = 0 \\rightarrow p=\\begin{pmatrix}P(Y=1|X=x)\\\\P(Y=0|X=x)\\end{pmatrix}=\\begin{pmatrix}0\\\\1\\end{pmatrix}\n",
    "\\label{eq: one-hot-vector} \\tag{3}\n",
    "\\end{equation}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we find a convex (hopefully) loss function that measures the \"distance\" between two distributions; the *binary cross-entropy* loss function is widely used. \\\n",
    "We define the average loss over $m$ examples as follows:\n",
    "\n",
    "$$\\begin{equation}\n",
    "J(w)=-\\frac{1}{m}{\\sum_{i=1}^{m}}{p_i}^{T}\\ln(\\hat{p}_i)\n",
    "\\label{eq: cross-entropy} \\tag{4}\n",
    "\\end{equation}$$\n",
    "\n",
    "Applying gradient descent **on the scaled features ($x$)** should converge to the *decision surface* ($w$).\n",
    "\n",
    "This type of classification is called *logistic regression*. **We remind you that it is not used for regression but for classification only!** This name is used due to historical reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define $n_x=D+1$ and then vectorize the process by stacking our examples in a matrix $X \\in \\mathbb{R}^{n_x x m}$ and thus $z=w^TX$. We can now define $a=\\sigma(z) \\in \\mathbb{R}^m$ and our update is calculated as follows:\n",
    "\n",
    "$$\\begin{equation}\n",
    "w_{n+1} = w_{n} - \\frac{\\alpha}{m}X(a-y)\n",
    "\\label{eq: GD} \\tag{5}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass classification\n",
    "Suppose we have $K$ categories. In this approach we actually learn a set of $K-1$ weights and predict our output as category $k$ for $k$ that maximizes the *softmax* function. This function is used to normalize the outputs as probabilities while emphasizes high-valued outputs and depresses low-valued ones. In total, if we define $n_y=K-1$ we get the following: \n",
    "\n",
    "$$\\begin{equation}\n",
    "W\\in \\mathbb{R}^{n_y x n_x}\\end{equation}$$\n",
    "\n",
    "$$\\begin{equation}z = Wx\\in \\mathbb{R}^{n_y}\\end{equation}$$\n",
    "\n",
    "$$\\begin{equation}a = softmax(z) = \\frac{e^z}{\\sum_{k=1}^{n_y}e^{z_i}}\n",
    "\\end{equation}$$\n",
    "\n",
    "Thus, our cost function is calculated as:\n",
    "\n",
    "$$\\begin{equation}\n",
    "J(W) = \\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{n_y}\\mathbb{1}^{T}\\{y^{(i)}=k\\}\\ln(a_{ik})\n",
    "\\label{eq: multicalss} \\tag{6}\n",
    "\\end{equation}$$\n",
    "\n",
    "Where $a_{ik}$ is the $k^{th}$ element of the softmax vector $a$ calculated on the $i^{th}$ example ($i^{th}$ column of the matrix $X$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mpl_toolkits import mplot3d\n",
    "from matplotlib import cm\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "warnings.filterwarnings('ignore')\n",
    "mpl.style.use(['ggplot']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/wdbc.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commonly encountered issue: Anonymization\n",
    "Hospital IT personnel extracted relevant patient data from the hospital EMR (Microsoft SQL server) and de-identified data by removing patients’ names and address. Specific views, i.e., tables containing multiple variables relating to a given type of medical data, were created. However, the hospital identifiers (ID) were left in and so you need to remove it and create a new patient  ID specific to your study. The mapping between the new ID and the hospital ID should be kept by the hospital personal to ensure anonymity. Here we don't (and we shouldn't) have the key and we can simply drop the ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(columns=['id','Unnamed: 32'],inplace=True) # annonimize and remove an irrelevant column\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, 1:31]\n",
    "Y = dataset.iloc[:, 0]\n",
    "Y.value_counts().plot(kind=\"pie\", labels=['B','M'], colors = ['steelblue', 'salmon'], autopct='%1.1f%%') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`seaborn` (imported as `sns`) is an easy and a great package for data visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dataset.loc[:,'diagnosis':'area_mean'], hue=\"diagnosis\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melt_plot(data,feat_name):\n",
    "    data_2_plot = pd.melt(data,id_vars=\"diagnosis\",\n",
    "                    var_name=feat_name,\n",
    "                    value_name='value')\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.swarmplot(x=feat_name, y=\"value\", hue=\"diagnosis\", data=data_2_plot, size=2)\n",
    "    plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `melt_plot` in order to compare the distributions. We should scale the data first and then compare the mean, SE and extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled = (X - X.mean()) / (X.std())  \n",
    "\n",
    "data_mean = pd.concat([Y, data_scaled.loc[:,'radius_mean':'fractal_dimension_mean']], axis=1)\n",
    "melt_plot(data_mean,\"features: mean\")\n",
    "\n",
    "data_SE = pd.concat([Y, data_scaled.loc[:,'radius_se':'fractal_dimension_se']], axis=1)\n",
    "melt_plot(data_SE,'features: SE')\n",
    "\n",
    "data_extreme = pd.concat([Y, data_scaled.loc[:,'radius_worst':'fractal_dimension_worst']], axis=1)\n",
    "melt_plot(data_extreme,'features: extreme')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important method to visualize the distribution of the data is by using their joint distribution using *kernel density plot*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.PairGrid(dataset.loc[:,'diagnosis':'smoothness_mean'], hue=\"diagnosis\")\n",
    "g.map_diag(sns.kdeplot)\n",
    "g.map_offdiag(sns.kdeplot);\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific task:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should divide our data into training and testing set in 80%-20% ratios respectively. As you saw earlier, the data is *imbalanced*, i.e. their labels ratios are not the same. We will use *stratification* to make sure that the ratio of the labels is preserved in both training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=10, stratify=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our naive classifier accuracy is simply the ratio of benign examples which is. A naive classifier (decision function, model etc.) is an estimator that its' prediction is always the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The naive classifier achieved an accuracy of %.2f%%.' % (100 * y_test.value_counts()['B']/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case the classifier would be \"correct\" in 63.16% of the cases if it will simply classify all examples as benign. This can be used as our baseline accuracy performance.\n",
    "\n",
    "Now we will use again one of the most powerful Python ML tools which is `scikit-learn`. Specifically, we will use the [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class.\n",
    "\n",
    "Create `log_reg` object of the class `LogisticRegression` and fit your training set. In `scikit-learn`, the bias is added as default. Once fit is done, the model weights were \"learned\" (estimated) and the model can perform other methods in belong to `LogisticRegression` class. Set `random_state` to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C1\n",
    "#----------------------Implement your code here:------------------------------\n",
    "\n",
    "#------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the accuracies of the classifier on both training and testing sets (take a look again at [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)). Display it with 2 digits after the decimal point as we did we the naive classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C2\n",
    "#----------------------Implement your code here:------------------------------\n",
    "\n",
    "#------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "96.04% for training.\n",
    "\n",
    "92.98% for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Question:***</span> *Explain the differences in training and testing.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look at the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C3\n",
    "plot_confusion_matrix(log_reg, X_test, y_test, cmap=plt.cm.Blues)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how standardization effects the results. Repeat the same instructions above (including plotting the confusion matrix) but now with scaling the data using `StandardScaler`. Create the scaled datasets and name them `X_train_scaled` and `X_test_scaled`. Repeat the stages C1-C3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C4\n",
    "#----------------------Implement your code here:------------------------------\n",
    "\n",
    "#-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected outpout:\n",
    "<center><img src=\"outputs/1.PNG\" width=\"380\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Question:***</span> *Compare the results of the confusion matrices. What are the clinical meaning of the improvement?*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have another look at the *melt plots* above.\n",
    "\n",
    "---\n",
    "<span style=\"color:red\">***Question:***</span> *Do you think that one of the groups (mean, SE, extreme) is redundant?*\n",
    "\n",
    "---\n",
    "\n",
    "If so, generate `X_train_selected` and `X_test_selected` by dropping that group and repeating the process. **Apply with scaling**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C5\n",
    "#----------------------Implement your code here:------------------------------\n",
    "\n",
    "#------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected outpout for 'radius_mean':'fractal_dimension_se'\n",
    "<center><img src=\"outputs/2.PNG\" width=\"380\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Question:***</span> *Did it help or did this group have some added information?*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will see if we can classify our data well with much less features. Extract 3 features that you think they are discriminative from training and testing sets. Make sure they are still in a `DataFrame` format.\n",
    "\n",
    "---\n",
    "<span style=\"color:red\">***Question:***</span> *What made you choose these specific features? If you chose one feature, can it \"eliminate\" the choice of other feature?*\n",
    "\n",
    "---\n",
    "\n",
    "Generate `X_train_3feat` and `X_test_3feat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C6\n",
    "#----------------------Implement your code here:------------------------------\n",
    "# radius, texture and smoothness\n",
    "\n",
    "#------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should encode `y_train` and `y_test` to one-hot vector (0 for benign and 1 for malignant). Here is a simple way to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = 1 * (y_train=='M')\n",
    "y_test = 1 * (y_test=='M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try classifying the diagnosis using only one feature. Choose one feature of the training set and convert it to `numpy` array. Do the same with the testing set. Then fit your linear model and calculate the accuracy upon the test set. Don't forget to add the correct xlable with units. Use the following notations: `X_train_1feat`, `X_test_1feat`, `y_train`, `y_test`, `log_reg`, `acc` and `xlbl`.  **Do not scale** `X_train_1feat` **and** `X_test1feat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C7\n",
    "#----------------------Implement your code here:------------------------------\n",
    "\n",
    "#------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_1D_classifier(x_train, x_test, y_train, y_test, log_reg, acc, xlbl=\"radius_worst [mm]\"):\n",
    "    \n",
    "    X_new = np.sort(x_train[:,0])\n",
    "    y_prob_train = log_reg.predict_proba(x_train)\n",
    "    y_prob_train = np.sort(y_prob_train[:, 1])\n",
    "    decision_boundary = X_new[y_prob_train >= 0.5][0]\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(x_test[y_test==0,0], y_test[y_test==0], \"bs\", label='B')\n",
    "    plt.plot(x_test[y_test==1,0], y_test[y_test==1], \"r^\", label='M')\n",
    "    \n",
    "    y_proba = log_reg.predict_proba(x_test)\n",
    "    y1 = np.sort(y_proba[:, 1])\n",
    "    y2 = np.sort(y_proba[:, 0])\n",
    "    x_test = np.sort(x_test[:,0])\n",
    "\n",
    "    plt.plot(x_test, y1, \"r-\", linewidth=2)\n",
    "    plt.plot(x_test, y2[::-1], \"b--\", linewidth=2)\n",
    "    plt.plot([decision_boundary, decision_boundary], [-0.1, 1.1], \"k:\", linewidth=2)\n",
    "    plt.text(decision_boundary+5, 0.5, \"Decision  boundary\", fontsize=14, color=\"k\", ha=\"center\")\n",
    "    plt.arrow(decision_boundary, 0.08, -0.3, 0, head_width=0.05, head_length=0.1, fc='b', ec='b')\n",
    "    plt.arrow(decision_boundary, 0.92, 0.3, 0, head_width=0.05, head_length=0.1, fc='r', ec='r')\n",
    "    plt.xlabel(xlbl, fontsize=14)\n",
    "    plt.ylabel(\"Probability\", fontsize=14)\n",
    "    plt.legend(loc=\"center left\", fontsize=14)\n",
    "    plt.title('Accuracy is %.2f ' % acc) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_1D_classifier(X_train_1feat, X_test_1feat, y_train, y_test, log_reg, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected outputs for 'radius_worst':\n",
    "<center><img src=\"outputs/3.PNG\" width=\"380\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*Take a look at the results. Make sure you understand them and it makes sense before you move on.*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now select two features that you think can be discriminative. Go through the same instructions as before but now you should also calculate carefully the decision boundary. Add the correct `xlbl` and `ylbl` and set the axes limits properly (`xmin`, `xmax`, `ymin`, `ymax`).\n",
    "Generate `X_train_2feat` and `X_test_2feat`. Don't forget to convert them to `numpy` arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C8\n",
    "#----------------------Implement your code here:------------------------------\n",
    "\n",
    "#------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2D_classifier(x_test, y_test, log_reg, boundary, acc, xlbl=\"radius_worst [mm]\", ylbl=\"compactness_worst [N.U]\", axes_lim=[7.5, 30, 0, 1]):\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(x_test[y_test==0, 0], x_test[y_test==0, 1], \"bs\", label='B')\n",
    "    plt.plot(x_test[y_test==1, 0], x_test[y_test==1, 1], \"r^\", label='M')\n",
    "    \n",
    "    x1, x2 = np.meshgrid(\n",
    "            np.linspace(x_test[:,0].min(),x_test[:,0].max(), len(y_test)).reshape(-1, 1),\n",
    "            np.linspace(x_test[:,1].min(),x_test[:,1].max(), len(y_test)).reshape(-1, 1),\n",
    "        )\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_proba = log_reg.predict_proba(X_new)\n",
    "    zz = y_proba[:, 1].reshape(x1.shape)\n",
    "    contour = plt.contour(x1, x2, zz, cmap=plt.cm.brg)\n",
    "    \n",
    "    plt.clabel(contour, inline=1, fontsize=12)\n",
    "    plt.plot(x_test[:,0], boundary, \"k--\", linewidth=3)\n",
    "    plt.xlabel(xlbl, fontsize=14)\n",
    "    plt.ylabel(ylbl, fontsize=14)\n",
    "    plt.title('Accuracy is %.2f ' % acc) \n",
    "    plt.legend(loc=\"upper left\", fontsize=14)\n",
    "    plt.axis(axes_lim)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2D_classifier(X_test_2feat, y_test, log_reg, boundary, acc, xlbl=xlbl, ylbl=ylbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected outputs for 'radius_worst' and 'compactness_worst':\n",
    "<center><img src=\"outputs/4.PNG\" width=\"380\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contours represent the iso probability contours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now select 3 features. Use `x1` and `x2` **as given** to calculate the boundary decision (a plane)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C9\n",
    "#----------------------Implement your code here:------------------------------\n",
    "\n",
    "#------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3D_classifier(x_test, y_test, x1, x2, boundary, acc, xlbl=\"radius_worst [mm]\", ylbl=\"compactness_worst [N.U]\", zlbl=\"concavity_worst [N.U]\", \n",
    "                       el=42, az=215):\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    ax = plt.axes(projection='3d')\n",
    "    \n",
    "    ax.scatter(x_test[y_test==0, 0], x_test[y_test==0, 1], x_test[y_test==0, 2], c=x_test[y_test==0, 2], cmap='Blues_r', label='B');\n",
    "    ax.scatter(x_test[y_test==1, 0], x_test[y_test==1, 1], x_test[y_test==1, 2], c=x_test[y_test==1, 2], cmap='Reds_r', label='M');\n",
    "    ax.plot_surface(x1, x2, boundary, cmap=cm.gray,\n",
    "                            linewidth=0, antialiased=False, alpha=0.5)\n",
    "    ax.view_init(el, az)\n",
    "    ax.set_xlabel(xlbl, fontsize=14)\n",
    "    ax.set_ylabel(ylbl, fontsize=14)\n",
    "    ax.set_zlabel(zlbl, fontsize=14)\n",
    "    plt.legend(loc=\"best\", fontsize=14)\n",
    "    plt.title('Accuracy is %.2f ' % acc) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change `el` and `az` as needed in order to see the decision surface properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3D_classifier(x_test, y_test, x1, x2, boundary, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected outputs for 'radius_worst', 'compactness_worst' and 'concavity_worst':\n",
    "<center><img src=\"outputs/5.PNG\" width=\"380\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*Notice the change in performance.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage in healthcare and conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this tutorial we:\n",
    ">- implemented a classifier to diagnose breast lumps.\n",
    "\n",
    ">- demonstrated how to implement logistic regression using `scikit-learn` built-in function.\n",
    "\n",
    ">- saw the impact of standardization on classification's performances.\n",
    "\n",
    ">- saw the impact of *feature selection*.\n",
    "\n",
    ">- used 1D, 2D and 3D figures to illustrate the separation between binary labels.\n",
    "\n",
    "#### Even though we deal with medical topics, there are still cases where the simplest models perform very well. Keep it in mind for the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images credit:\n",
    "* [Mammograms](https://www.cancer.gov/cancertopics/screening/understanding-breast-changes/page6)\n",
    "* [2D classification](https://www.joyofdata.de/blog/wp-content/uploads/2014/04/lin-sep-points-2d-10.png)\n",
    "* [3D classification gif](https://www.kdnuggets.com/2019/09/friendly-introduction-support-vector-machines.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *This tutorial was written by [Moran Davoodi](mailto:morandavoodi@gmail.com) with the assitance of [Yuval Ben Sason](mailto:yuvalbse@gmail.com), Kevin Kotzen & Alon Begin*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
