{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BME-336546-C11-Hyperparameters optimization in neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medical topic\n",
    "Refer to previous tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Refer to previous tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main ML topic:\n",
    "Hyperparameters optimization through validation of CNN's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory reminders\n",
    "A nice visualiization that shows the linkage between the gradient descent in the error surface and the goodness of fit is shown here:\n",
    "<center><img src=\"images/gif.gif\" width=400><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we remember we use scaling mostly due to sensitivity of gradient descent to scaling and/or biased learning.\n",
    "<center><img src=\"images/grad_descent.png\" width=400><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In neural networks, we have multiple layers that we need to scale correctly. How should we do that? We can actually *learn* the scaling and shifting factors. Mostly, we will define trainable parameters $\\beta$ and $\\gamma$ per layer and every batch in the layer is standardized as follows:\n",
    "<center><img src=\"images/batch_normalization.png\" width=300><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, these parameters also affect the loss function $(L(w) \\rightarrow L(w,\\beta, \\gamma))$ and they can be updated by gradient descent. Training become much faster by using batch normalization.\n",
    "\n",
    "There are two more main factors that affect the gradient descent process:\n",
    "* Stochastic vs. minibatch.\n",
    "* Constant learning rate vs. adaptive one.\n",
    "\n",
    "Here is a quick summary of stochastic vs. minibatch:\n",
    "\n",
    "\n",
    "<left><img src=\"images\\batch_stoch_2.png\" width=\"400\"><left>\n",
    "<right><img src=\"images\\batch_stoch.png\" width=\"400\"><right>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the momentum gradient descent, we add another term to the constant learning rate that depends on the previous step. Thus, if the current step is \"at the same direction\" as the previous one, the weights would be \"pushed\" towards there with larger learning rate. If not, then it would be regulated by the new step.  \n",
    "$$\\begin{align}\n",
    "v_{n+1} &= \\mu v_n - \\alpha \\frac{\\partial{L}}{\\partial{W_n}}\\\\\n",
    "W_{n+1} &= W_n + v_{n+1}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in neural networks, we have a lot of hyperparameters (degrees of freedom) that we need to tune or set. Among them we can find the following:\n",
    "* Batch size.\n",
    "* Number of epochs.\n",
    "* Types of regularization.\n",
    "* Weights' initialization.\n",
    "* Learning rate types and magnitude.\n",
    "* Activation functions.\n",
    "* Number of neurons or filters in every layer.\n",
    "* Network's depth (number of layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "The datasets are located on a shared file on triton at `/MLdata/MLcourse/LTAF/`. Let's convert this notebook into `.py` file or work directly with the `ipynb` file and check out the benefits of PyCharm Professional when it comes to working with remote servers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.style.use(['ggplot']) \n",
    "# %matplotlib inline\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from IPython.display import display, clear_output\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL']='3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, load_model \n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Conv1D, MaxPool1D, Flatten, BatchNormalization\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_src = '/MLdata/MLcourse/LTAF/'\n",
    "y = np.load(data_src + 'y_LTAF.npy')\n",
    "rr = np.load(data_src + 'rr_LTAF.npy')\n",
    "\n",
    "rr_train, rr_test, y_train, y_test = train_test_split(rr, y, test_size = 0.20, random_state = 336546, stratify=y)\n",
    "# rr_train, rr_val, y_train, y_val = train_test_split(rr_train_orig, y_train_orig, test_size = 0.20, random_state = 336546, stratify=y_train_orig)\n",
    "\n",
    "# rr_train_orig = rr_train_orig.reshape(rr_train_orig.shape[0],rr_train_orig.shape[1],1)\n",
    "rr_train = rr_train.reshape(rr_train.shape[0],rr_train.shape[1],1)\n",
    "# rr_val = rr_val.reshape(rr_val.shape[0],rr_val.shape[1],1)\n",
    "rr_test = rr_test.reshape(rr_test.shape[0],rr_test.shape[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
    "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific task: Tune your neural network\n",
    "Build the same neural network as in previous tutorial inside `create_model` and add batch normalization between every convolution/dense layer. Don't change `window_size` and `len_Sub_window`. Choose only 3 hyperparmeters to tune and set no more than 2 options for every hypeparmeter. Notice that the hyparameters of batch size, number of epochs and weights' initialization are external to the model itself and do not count as arguments of the function `create_model`. However, you can include it in the grid search. If you choose not to tune the hyperparameters of batch size and/or epochs, then you must define them in `KerasClassifier`. Use more guidence [here](https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(window_size=60, len_sub_window=10,n_filters_start=64,n_hidden_start=512,dropout=0.5,lr=0.01, momentum=0):\n",
    "    #----------------------Implement your code here:------------------------------\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(n_filters_start, len_sub_window, activation='relu', input_shape=(60, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(2 * n_filters_start, len_sub_window, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool1D())\n",
    "    model.add(Conv1D(4 * n_filters_start, len_sub_window, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(n_hidden_start, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(int(n_hidden_start / 2), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(int(n_hidden_start / 4), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))   \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    optimizer = SGD(lr=lr, momentum=momentum)\n",
    "    model.compile(optimizer=optimizer, metrics=['accuracy'], loss='binary_crossentropy')\n",
    "    #------------------------------------------------------------------------------\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model,verbose=1, epochs=30)\n",
    "batch_size = [2000, 4000]\n",
    "n_filters_start = [32, 64, 128]\n",
    "dropout = [0.1, 0.2, 0.5]\n",
    "param_grid = dict(batch_size=batch_size, n_filters_start=n_filters_start, dropout=dropout)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result = grid.fit(rr_train, y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit your best model with the complete training set, name it `final_model` and evaluate on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------Implement your code here:------------------------------\n",
    "final_model = KerasClassifier(build_fn=create_model,verbose=1, epochs=30, **grid_result.best_params_)\n",
    "final_model.fit(rr_train, y_train)\n",
    "#------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "calc_TN = lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[0, 0]\n",
    "calc_FP = lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[0, 1]\n",
    "calc_FN = lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[1, 0]\n",
    "calc_TP = lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_metric(y_test, y_pred_test, y_pred_proba_test, clf_name, temp=np.empty(())):\n",
    "    TN = calc_TN(y_test, y_pred_test)\n",
    "    FP = calc_FP(y_test, y_pred_test)\n",
    "    FN = calc_FN(y_test, y_pred_test)\n",
    "    TP = calc_TP(y_test, y_pred_test)\n",
    "    Se = TP/(TP+FN)\n",
    "    Sp = TN/(TN+FP)\n",
    "    PPV = TP/(TP+FP)\n",
    "    NPV = TN/(TN+FN)\n",
    "    Acc = (TP+TN)/(TP+TN+FP+FN)\n",
    "    F1 = (2*Se*PPV)/(Se+PPV)\n",
    "    print('The fitted classifier is ' + clf_name + '\\n')\n",
    "    print('Sensitivity is {:.2f}. \\nSpecificity is {:.2f}. \\nPPV is {:.2f}. \\nNPV is {:.2f}. \\nAccuracy is {:.2f}. \\nF1 is {:.2f}. '.format(Se,Sp,PPV,NPV,Acc,F1))\n",
    "    if temp.size == 1:\n",
    "        print('AUROC is {:.2f}'.format(roc_auc_score(y_test, y_pred_proba_test[:,1])))\n",
    "    else:\n",
    "        print('AUROC is {:.2f}'.format(roc_auc_score(y_test, temp[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = final_model.predict(rr_test)\n",
    "y_pred_test[y_pred_test>=0.5] = 1\n",
    "y_pred_test[y_pred_test<0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = final_model.predict(rr_test)\n",
    "temp2 = np.zeros((temp.shape[0], 2))\n",
    "temp2[:,0] = 1-temp[:,0]\n",
    "temp2[:,1] = temp[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_metric(y_test, y_pred_test, y_pred_proba_test, clf_name='CNN', temp=temp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images credit:\n",
    "* [batch vs. stochastic](https://www.kdnuggets.com/2016/08/gentlest-introduction-tensorflow-part-2.html/2)\n",
    "* [batch vs. stochastic2](https://xzz201920.medium.com/gradient-descent-stochastic-vs-mini-batch-vs-batch-vs-adagrad-vs-rmsprop-vs-adam-3aa652318b0d)\n",
    "* [gif](https://towardsdatascience.com/improving-vanilla-gradient-descent-f9d91031ab1d)\n",
    "* [scaling](https://www.commonlounge.com/discussion/fc3fb95081e54ab3b00368aacbdc62be/history)\n",
    "* [batch normalization](https://arxiv.org/pdf/1502.03167v3.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *This tutorial was written by [Moran Davoodi](mailto:morandavoodi@gmail.com) with the assitance of [Yuval Ben Sason](mailto:yuvalbse@gmail.com) & Kevin Kotzen*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's all folks! Hope you enjoyed the course :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
