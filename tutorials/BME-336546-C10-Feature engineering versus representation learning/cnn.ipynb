{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BME-336546-C10-Feature engineering versus representation learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medical topic\n",
    "This tutorial's medical topic is atrial fibrillation (AF) arrhythmia detection. We will rely on features which were found and extracted in our lab and pubilshed [here](https://ieeexplore.ieee.org/document/9281068/authors#authors). Some of the features and the model we will build today are shown below below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/Capture3.png\" width=400><center>\n",
    "<center><img src=\"images/Capture1.PNG\" width=400><center>\n",
    "<center><img src=\"images/Capture2.PNG\" width=300><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our mission\n",
    "In this tutorial we will compare between the classical supervised machine learning, which is based on classification using features extracted by humans, and deep learning which is based on feature extraction and classification done by neural network that is fed by raw data. Moreover, we will learn how to use our faculty's GPU clusters. Check [BME cluster Wiki](http://132.68.176.116/index.php/Main_Page), written by Snir Lugassy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.style.use(['ggplot']) \n",
    "# %matplotlib inline\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from IPython.display import display, clear_output\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import log_loss\n",
    "import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL']='3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, load_model \n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Conv1D, MaxPool1D, Flatten \n",
    "from tensorflow.keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_src = '/MLdata/MLcourse/LTAF/'\n",
    "X = np.load(data_src + 'X_LTAF.npy')\n",
    "y = np.load(data_src + 'y_LTAF.npy')\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 336546, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "calc_TN = lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[0, 0]\n",
    "calc_FP = lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[0, 1]\n",
    "calc_FN = lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[1, 0]\n",
    "calc_TP = lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_metric(y_test, y_pred_test, y_pred_proba_test, clf_name, temp=np.empty(())):\n",
    "    TN = calc_TN(y_test, y_pred_test)\n",
    "    FP = calc_FP(y_test, y_pred_test)\n",
    "    FN = calc_FN(y_test, y_pred_test)\n",
    "    TP = calc_TP(y_test, y_pred_test)\n",
    "    Se = TP/(TP+FN)\n",
    "    Sp = TN/(TN+FP)\n",
    "    PPV = TP/(TP+FP)\n",
    "    NPV = TN/(TN+FN)\n",
    "    Acc = (TP+TN)/(TP+TN+FP+FN)\n",
    "    F1 = (2*Se*PPV)/(Se+PPV)\n",
    "    print('The fitted classifier is ' + clf_name + '\\n')\n",
    "    print('Sensitivity is {:.2f}. \\nSpecificity is {:.2f}. \\nPPV is {:.2f}. \\nNPV is {:.2f}. \\nAccuracy is {:.2f}. \\nF1 is {:.2f}. '.format(Se,Sp,PPV,NPV,Acc,F1))\n",
    "    if temp.size == 1:\n",
    "        print('AUROC is {:.2f}'.format(roc_auc_score(y_test, y_pred_proba_test[:,1])))\n",
    "    else:\n",
    "        print('AUROC is {:.2f}'.format(roc_auc_score(y_test, temp[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with feature engineering. Scale your training and testing sets. Take the scaled data and fit the next three models onto it:\n",
    "*  Logistic regression with $L_2$ penalty and maximum number of iterations of 300.\n",
    "*  SVM with rbf kernel and $C=1$ and maximum number of iterations of 1000.\n",
    "*  Random forest with 20 estimators and maximal depth of 5.\n",
    "For each and every one of them, calculate the predictions of the test set and name the prediction `y_pred_test`. You should also calculate the probabilites and name it `y_pred_proba_test` except the SVM case. For each model, run the function `stat_metric`. Set all `random_state` to 336546.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "#----------------------Implement your code here:------------------------------\n",
    "\n",
    "#------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_metric(y_test, y_pred_test, y_pred_proba_test, clf_name='logistic regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "<center><img src=\"outputs/1.PNG\" width=\"480\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "#----------------------Implement your code here:------------------------------\n",
    "\n",
    "#------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_metric(y_test, y_pred_test, y_pred_proba_test, clf_name='RBF SVM') # y_pred_proba_test is not really calculated here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "<center><img src=\"outputs/2.PNG\" width=\"480\"><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#----------------------Implement your code here:------------------------------\n",
    "\n",
    "#------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_metric(y_test, y_pred_test, y_pred_proba_test, clf_name='random forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "<center><img src=\"outputs/3.PNG\" width=\"480\"><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = np.load(data_src + 'rr_LTAF.npy')\n",
    "print(rr.shape) #84 patients, 1700 windows per each patient. 60 beats per each window. 142853 windows in total\n",
    "rr_train, rr_test, _, _ = train_test_split(rr, y, test_size = 0.20, random_state = 336546, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
    "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the following model:\n",
    "conv1d with 128 filters with 10 smaples size --> maxpool --> conv1d with 256 filters with 10 smaples size --> dorpout with probabilty of 0.5 --> Flatten --> 3 fully connected hidden layers with relu activation and 512, 256 and 128 neurons respectively --> dropuot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size=60\n",
    "n_filters_start=64\n",
    "n_hidden_start=512\n",
    "len_sub_window=10\n",
    "dropout=0.5\n",
    "model = Sequential()\n",
    "model.add(Conv1D(n_filters_start, len_sub_window, activation='relu', input_shape=(60, 1)))\n",
    "#----------------------Implement your code here:------------------------------\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# model.add(Dense(2, activation='softmax')) # should change the labels for that\n",
    "model.compile(optimizer='adam', metrics=['accuracy'], loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "<center><img src=\"outputs/4.PNG\" width=\"480\"><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_train = rr_train.reshape(rr_train.shape[0],rr_train.shape[1],1)\n",
    "rr_test = rr_test.reshape(rr_test.shape[0],rr_test.shape[1],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build the fitting model without running the cell. Use batch size of 1024 and 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------Implement your code here:------------------------------\n",
    "\n",
    "#-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(\"results\" in os.listdir()):\n",
    "    os.mkdir(\"results\")\n",
    "save_dir = \"results/\"\n",
    "model_name = \"final_weights.h5\"\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = load_model(\"results/final_weights.h5\")\n",
    "loss_and_metrics = final_model.evaluate(rr_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = final_model.predict(rr_test)\n",
    "y_pred[y_pred>=0.5] = 1\n",
    "y_pred[y_pred<0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = final_model.predict(rr_test)\n",
    "temp2 = np.zeros((temp.shape[0], 2))\n",
    "temp2[:,0] = 1-temp[:,0]\n",
    "temp2[:,1] = temp[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_metric(y_test, y_pred_test, y_pred_proba_test, clf_name='CNN', temp=temp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *This tutorial was written by [Moran Davoodi](mailto:morandavoodi@gmail.com) with the assitance of [Yuval Ben Sason](mailto:yuvalbse@gmail.com) & Kevin Kotzen*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowladgements:\n",
    "Thanks to our lab colleagues *Armand Chocron* and *Shany Biton* for helping with this tutrial which relies on their paper."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
