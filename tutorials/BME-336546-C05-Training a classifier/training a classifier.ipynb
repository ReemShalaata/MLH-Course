{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BME-336546-C05-Training a classifier\n",
    "\n",
    "Before we begin, please make sure you have updated the `bm-336546` using `tutorial5.yml`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medical topic\n",
    "Diabetic retinopathy is the most common diabetic eye disease and a leading cause of blindness in American adults. In the majority of diabetic retinopathy cases, blindness is completely preventable. Patients will need to work closely with their Maine Eye Center diabetic retina eye doctors to contain this disease. The use of medications and daily blood sugar monitoring can make a major impact on containing the worsening of diabetic retinopathy.\n",
    "\n",
    "The retina is the like the film in a camera; in which it is the light sensing film on the back of the eye that captures the images. In the diabetes disease, sugar (glucose) builds up within blood vessels in the retina and tissues of the body causing glucose to attach to the proteins in the wall. This alters their normal structure and functioning. The vessels eventually get blocked and leak fluid. When they cannot deliver an adequate amount of blood supply to the eye, the eye can generate abnormal new blood vessels. Early diabetic retinopathy usually has no symptoms. However, worsening diabetic retinopathy can lead to visual loss and blindness.\n",
    "Retinal exudates yellow flecks made up of lipid residues of serious leakage from damaged capillaries.\n",
    "\n",
    "Diabetes main cause, but also neuroretinitis, retinal vein occlusion,Von Hippel-Lindau Disease, other vascular dysplasias, radiation-induced retinal vasculopathy\n",
    "\n",
    "<center><img src=\"images/Diabetic-Retinopathy.png\" width=400><center>\n",
    "<center><img src=\"images/real diabetic retina.png\" width=400><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "`X`:\n",
    "* Number of microaneurysms (MA).\n",
    "* Exudates are represented by a set of points rather than the number of pixels constructing the lesions, these features are normalized by dividing the number of lesions with the diameter of the ROI to compensate different image sizes.\n",
    "* The euclidean distance of the center of the macula and the center of the optic disc to provide important information regarding the patient's condition. This feature is also normalized with the diameter of the ROI.\n",
    "* The diameter of the optic disc.\n",
    "\n",
    "`y`:\\\n",
    "Class label. 1 = contains signs of DR (Accumulative label for the Messidor classes 1, 2, 3), 0 = no signs of DR.\n",
    "\n",
    "[Data credit](https://archive.ics.uci.edu/ml/datasets/Diabetic+Retinopathy+Debrecen+Data+Set)\\\n",
    "[Paper credit](https://core.ac.uk/download/pdf/161019842.pdf)\n",
    "\n",
    "## Main ML topic:\n",
    "Our main ML topic here is still supervised learning. Here we will see how to evaluate our models, compare their performances and indicate their limitaions.\n",
    "\n",
    "## Our mission\n",
    "Evaluate linear models of DR detection and compare their perfomances later on to non-linear models.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory reminders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Bias/Variance tradeoff\n",
    "An important theoretical result of statistics and ML is the fact that a model's generalization error can be expressed as the sum of three very different errors:\n",
    "> ***Bias:*** This part of generalization error is due to wrong assumptions, such as assuming that the data is linear when it is actually quadratic. A high-bias model is most likely to underfit the training data.\n",
    "\n",
    "> ***Variance:*** This part of generalization error is due to the Model's excessive sensitivity to small variations in the training data. A model with many degrees of freedom (such as a high-degree polynomial model) is likely to have high variance, and thus to overfit the training data.\n",
    "\n",
    "> ***Irreducible error:*** This part is due to the noisiness of the data itself or the lack of representative model in our hypotheses class. The only way to partially reduce this error (only in the aspect of noise) is to clean up the data (e.g. fix the data sources, such as broken sensors, or detect and remove outliers).\n",
    "\n",
    "Increasing a model's complexity will typically increases its variance and reduce its bias. Conversely, reducing a model's complexity increases its bias and reduces its variance. This is why it is called **tradeoff**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/u_o_fitting class.png\" width=\"450\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/u_o_fitting regression.png\" width=\"600\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly, our cost functions alone are at high risk of overfitting since they only aim to fit the model to the data. In order to avoid this situation we can constrain our optimization problem to some smaller set of possible solutions by introducing a *regularization term* $E(w)$. The model parameters should now converge to a solution that balances the two terms. Thus, in our case the new cost function would look like this:\n",
    "\n",
    "$$\\begin{equation}\n",
    "J(W) = \\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{n_y}\\mathbb{1}^{T}\\{y^{(i)}=k\\}\\ln(a_{ik}) + E(W)\n",
    "\\label{eq: regularization} \\tag{7}\n",
    "\\end{equation}$$\n",
    "\n",
    "where the regularization term mostly consists of a variation of the $\\ell_p$ norm which is defined on a vector $w$ as its p-norm: $||w||_p=({\\sum_{j=1}^{n_x}|w_j|^p})^{1/p}$. In total our cost function is defied as:\n",
    "\n",
    "$$\\begin{equation}\n",
    "J(W) = \\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{n_y}\\mathbb{1}^{T}\\{y^{(i)}=k\\}\\ln(a_{ik}) + \\lambda\\sum_{k=1}^{n_y}||W^{(k)}||^p_p\n",
    "\\label{eq: lp} \\tag{8}\n",
    "\\end{equation}$$\n",
    "\n",
    "where $W^{(k)}$ is the $k^{th}$ row of the matrix $W$ and $\\lambda$ is a hyperparameter that sets the penalization. \n",
    "The higher $\\lambda$ is, the higher importance of the regularization term is, and thus the model will be pushed towards lower weights in order to minimize the cost function $J$. \\\n",
    "When $p=2$ the regularization is known as *Ridge regularization* and when $p=1$ it is known as *Lasso regularization*. Regularization can also be performed in a regression task, where it is supposed to achieve the optimal weights values of the **chosen** order of the polynomial model for instance. Thus the order of the polynomial model is another hyperparameter. Here is a graphical interpretation in the case of regularized **regression** in $2D$:\n",
    "<center><img src=\"images/ridge_lasso.PNG\" width=\"380\"><center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Question:***</span> *Which image represents Ridge regularization and which one is Lassso?*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The most important thing in learning is the *capability of generalization*. In general, we **fit** our model to the **training** set and once fitted, we **apply** the fitted model on the **testing** set which is a set that our model has never \"seen\" before. Capability of generalization is estimated by similarity of the loss function applied on each set. The more similar they are, the better the generalization is. Proper generalization should be achieved if the following conditions are satisfied:\n",
    "* Training and testing sets were sampled from the same distribution (population).\n",
    "* The training set has enough examples to use as a representative sample of the population.\n",
    "* The model is not overfitted.\n",
    "\n",
    "There are two risks in overfitting:\n",
    "* The model does not *learn* but *memorize*.\n",
    "* The model was fitted to the noise within our data.\\\n",
    "Overfitting and underfitting are actually a measure of the *bias-variance tradeoff*:\n",
    "<p style='text-align: center;'> Underfitting $\\rightarrow$ high bias and low variance.</p>\n",
    "<p style='text-align: center;'> Overfitting $\\rightarrow$ low bias and high variance.</p>\n",
    "\n",
    "This tradeoff is controlled by two main consecutive steps:\n",
    "* First, the complexity of the model (such as the degree of fitted polynom in regression or total number of features) should be set properly.\n",
    "* Once complexity was set, regularization should be applied.\n",
    "\n",
    "<center><img src=\"images/fitting.png\" width=500><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same concept applies also to classification tasks. Though in linear models, we don't really have different complexities, so we are left with choosing our type of regularization and the value of the hyperparameter $\\lambda$.\n",
    "How do we choose the type of regularization and $\\lambda$? $\\rightarrow$ **Validation**.\\\n",
    "First, we should pick a set of types of regularization and a set of hyperparameters.\\\n",
    "After you splitted your data into training and testing set, you should split also your training into a new *training* set and a *validation* set. The model will be fitted on the new training set and will be tested (applied) on the validation set. We will measure the performance for each hyperparmeter and each type and plot validation loss vs. $\\lambda$ and training loss vs. $\\lambda$. \\\n",
    "The $\\lambda$ value will be chosen as the \"sweet spot\" where both training loss and validation loss are minimized. Usually we would use K cross-fold validation as shown below:\n",
    "<center><img src=\"images/kfolds.png\" width=500><center>\n",
    "\n",
    "Once we tuned our model parmeters, we can now train the **whole** original training set and apply it on the testing set. The general process is summarized here:\n",
    "<center><img src=\"images/06_02.png\" width=500><center>\n",
    "\n",
    "In order to assess whether or not we have enough examples, we should plot a *learning curve* such as the ones you have seen in the lecture:\n",
    "<center><img src=\"images/learning curve.PNG\" width=500><center>\n",
    "\n",
    "Sometimes, instead of minimizing the loss, we can maximize performance. Untill now, we dealt only with accuracy performance but this type of \"scoring\" method can be misleading.\\\n",
    "For instance, we can consider a classification algorithm with an accuracy of 90% to be highly accurate algorithm. However, if our data was imbalanced with a ratio of 90%-10%, even a naive classifier would achieve that perofromance.\\\n",
    "Let's define some other metrics of statistical performance:\n",
    "<center><img src=\"images/confusion_matrix.png\" width=500><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sensitivity (𝑆𝑒): Proportion of people with a condition who are correctly identified by a test as indeed having that condition.\n",
    "* Specificity (𝑆𝑝): Proportion of people without a condition who are correctly identified by a test as indeed not having the condition.\n",
    "* Positive Predictive Value (𝑃𝑃𝑉): Probability that people with a positive test result indeed do have the condition of interest.\n",
    "* Negative Predictive Value (𝑁𝑃𝑉): Probability that people with a negative test result indeed do not have the condition of interest.\n",
    "\n",
    "Given a confusion matrix, the accuracy is calculated as $\\frac{TP+TN}{TP+FP+TN+FN}$.\\\n",
    "Another important metric is called $F_1$ which is the harmonic mean between *Se* and *PPV*: $F_1=2\\frac{PPV\\bullet Se}{PPV + Se}$. It is useful as a single measure for classifier optimization mostly when we have imbalanced data.\n",
    "\n",
    "One last measure of performance which is widely useful is the are under the receiver operating characteristic (ROC) curve or for short: *AUROC*. This measure tries to capture **how well our algorithm discriminates between two classes**. \\\n",
    "In the image below, consider *TPR* to be *Se* and *FPR* to be *1-Sp*. \n",
    "<center><img src=\"images/auroc.png\" width=500><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <center><img src=\"images/roc_intuition.png\" width=500><center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.style.use(['ggplot']) \n",
    "%matplotlib inline\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/objs.pkl','rb') as f: \n",
    "    X, y, _ = pickle.load(f) # pandas objects!\n",
    "max_iter = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing and exploration\n",
    "Let's plot the ratio of the classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts().plot(kind=\"pie\", labels=['No DR','DR'], colors = ['steelblue', 'salmon'], autopct='%1.1f%%') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we should convert the objects `X` and `y` to `numpy` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.values\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific task:\n",
    "We will start with splitting our data to training and testing set and we keep tracking the shuffled indices. Furthermore, we will define a technical function for choosing an adequate solver for different penalties of `LogisticRegression` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "X_train, x_test, Y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 10, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_penalty(penalty='none'):\n",
    "    if penalty == 'l1':\n",
    "        solver='liblinear'\n",
    "    if penalty == 'l2' or penalty == 'none':\n",
    "        solver='lbfgs'\n",
    "    return solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will split the **training set** into training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_orig, x_val_orig, y_train, y_val= train_test_split(X_train, Y_train, test_size = 0.20, random_state = 10, stratify=Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once splitted, scale your training set and validation set and name them `x_train` and `x_val` respectively. Notice that scaling is applied only **after** splitting in order to avoid information leakage. Train a *logistic regression* model with no regularization and then apply the trained model upon the validation set. Set `random_state` to be 5 and use the given `max_iter`. Compare their loss values. Use the imported `log_loss` function. Notice what are the inputs of this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C1\n",
    "scaler = StandardScaler()\n",
    "solver = check_penalty(penalty='none')\n",
    "#----------------------Implement your code here:------------------------------\n",
    "\n",
    "#-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "<center><img src=\"outputs/1.PNG\" width=\"380\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to check $l_2$ regularization effect on a coarse scale. Calculate and plot the loss over training and testing sets along given `lmbda`. $\\lambda$ axis is scaled by the logarithm with base of 10 when plotted. Notice what the argument `C` of `LogisticRegression` expects to recieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C2\n",
    "lmbda = np.array([0.0001, 0.001, 0.01, 1, 10, 100, 1000, 10000])\n",
    "pen = 'l2'\n",
    "solver = check_penalty(penalty=pen)\n",
    "J_train = np.zeros_like(lmbda)\n",
    "J_val = np.zeros_like(lmbda)\n",
    "with tqdm(total=len(lmbda), file=sys.stdout) as pbar:\n",
    "    for idx, lb in enumerate(lmbda):\n",
    "        pbar.set_description('processed: %d/%d' % ((1 + idx), len(lmbda)))\n",
    "        pbar.update(1)\n",
    "        #----------------------Implement your code here:------------------------------\n",
    "\n",
    "        #-----------------------------------------------------------------------------\n",
    "plt.plot(np.log10(lmbda), J_train)\n",
    "plt.plot(np.log10(lmbda), J_val)\n",
    "plt.xlabel('$\\log_{10}(\\lambda)$')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['J_train (n = ' + str(x_train.shape[0]) + ')', 'J_val (n = ' + str(x_val.shape[0]) + ')'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "<center><img src=\"outputs/2.PNG\" width=\"380\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you tell? Will regularization have any effect?\n",
    "\n",
    "Let's tune our regularization range according to the results above but still include $\\lambda$=0 in order to include no regularization option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C3\n",
    "fine_lmbda = np.arange(0,10,1, dtype=np.float64)\n",
    "J_train = np.zeros_like(fine_lmbda)\n",
    "J_val = np.zeros_like(fine_lmbda)\n",
    "with tqdm(total=len(fine_lmbda), file=sys.stdout) as pbar:\n",
    "    for idx, lb in enumerate(fine_lmbda):\n",
    "        pbar.set_description('processed: %d/%d' % ((1 + idx), len(fine_lmbda)))\n",
    "        pbar.update(1)\n",
    "        if lb-0.01 < 0:\n",
    "            C = np.inf # equivalent to 'none' penalty\n",
    "        else:\n",
    "            C = 1/lb\n",
    "        #----------------------Implement your code here:------------------------------\n",
    "\n",
    "        #-----------------------------------------------------------------------------\n",
    "plt.plot(fine_lmbda, J_train)\n",
    "plt.plot(fine_lmbda, J_val)\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('Loss')\n",
    "# plt.ylim(0.62, 0.68)\n",
    "plt.legend(['J_train (n = ' + str(x_train.shape[0]) + ')', 'J_val (n = ' + str(x_val.shape[0]) + ')'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "<center><img src=\"outputs/3.PNG\" width=\"380\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important technical point to consider: Try to run the same code you implemented but **without** declaring `dtype=np.float64` in `fine_lambda`. What is the result now? Why do you think it happened? After a quick discussion run the cell again **with** declaring `dtype=np.float64` and move on to the next part.\n",
    "\n",
    "Now we would like to plot our *learning curve* to find out if we should have had more data. First choose the best $\\lambda$ value. Then, in each iteration, train your model on an increasing training data data set and apply the model on `x_val`. Set `random_state` in `train_test_split` and don't forget to use stratification. Think what should be the `test_size` in each iteration w.r.t `curr_ratio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# choose best lmbda\n",
    "best_lmbda = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C4\n",
    "data_ratio = np.linspace(0.0028,0.998,num = 100)\n",
    "J_train = np.zeros_like(data_ratio)\n",
    "J_val = np.zeros_like(data_ratio)\n",
    "log_reg = LogisticRegression(random_state=5, penalty=pen, C = 1/best_lmbda, max_iter=max_iter,solver=solver)\n",
    "with tqdm(total=len(data_ratio)-1, file=sys.stdout) as pbar:\n",
    "    for idx, curr_ratio in enumerate(data_ratio):\n",
    "        pbar.set_description('processed: %d/%d' % ((1 + idx), len(data_ratio)))\n",
    "        pbar.update(1)\n",
    "        #----------------------Implement your code here:------------------------------\n",
    "\n",
    "        #-----------------------------------------------------------------------------\n",
    "fig, axes = plt.subplots(1,1,figsize=(10,10))\n",
    "axes.plot(np.ceil(data_ratio[5:]*x_train.shape[0]), J_train[5:])\n",
    "axes.plot(np.ceil(data_ratio[5:]*x_train.shape[0]), J_val[5:])\n",
    "axes.set_xlabel('# of examples in training set')\n",
    "axes.set_ylabel('Loss')\n",
    "axes.legend(['J_train', 'J_val (n = ' + str(x_val.shape[0]) + ')'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "<center><img src=\"outputs/4.PNG\" width=\"380\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Question:***</span> *What are your conclusions? Would more data help us? In addition, what is the term that we use in order to describe the left side of the plot?*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use *K-fold cross validation* to have more robustic conclusions. Our plots are basically the errorbars of the loss for every $\\lambda$ where the standard deviation is used as the error. **Pay close attention to which data is used in** `skf.split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C5\n",
    "lmbda = np.array([0.0001, 0.001, 0.01, 1, 10, 100, 1000, 10000])\n",
    "# fine_lmbda = np.linspace(0.1,5,num=20) # after seeing cross-fold on logscale\n",
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, random_state=10, shuffle=True)\n",
    "\n",
    "J_train = np.zeros((2,len(lmbda)))\n",
    "J_val = np.zeros((2,len(lmbda)))\n",
    "\n",
    "# pen = 'l2'\n",
    "solver = check_penalty(penalty=pen)\n",
    "\n",
    "for idx, lmb in enumerate(lmbda):\n",
    "    C = 1/lmb\n",
    "    #--------------------------Impelment your code here:-------------------------------------\n",
    "\n",
    "    #----------------------------------------------------------------------------------------\n",
    "    with tqdm(total=n_splits, file=sys.stdout, position=0, leave=True) as pbar:\n",
    "        h = 0 # index per split per lambda\n",
    "        J_train_fold = np.zeros(n_splits)\n",
    "        J_val_fold = np.zeros(n_splits)\n",
    "\n",
    "        for train_index, val_index in skf.split(X_train, Y_train):\n",
    "            pbar.set_description('%d/%d lambda values, processed folds' % ((1 + idx), len(lmbda)))\n",
    "            pbar.update()\n",
    "            #--------------------------Impelment your code here:-------------------------------------\n",
    "\n",
    "            #----------------------------------------------------------------------------------------\n",
    "            h += 1\n",
    "        #--------------------------Impelment your code here:-------------------------------------\n",
    "\n",
    "        #----------------------------------------------------------------------------------------\n",
    "plt.errorbar(np.log10(lmbda), J_train[0,:], yerr=J_train[1,:])\n",
    "plt.errorbar(np.log10(lmbda), J_val[0,:], yerr=J_val[1,:]) \n",
    "plt.xlabel('$\\log_{10}\\lambda$')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['J_train (n = ' + str(x_train_fold.shape[0]) + ')', 'J_val (n = ' + str(x_val_fold.shape[0]) + ')'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "<center><img src=\"outputs/5.PNG\" width=\"380\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would assume that we would have the same results for a finer range and thus we will choose our best $\\lambda$ to be the same again.\\\n",
    "`max_data_ratio` is a function that returns the maximal range of number of examples to train on according to the number of folds (`n_splits`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_data_ratio(n_splits,x_train,y_train):\n",
    "    data_ratio_orig = np.linspace(0.01,0.98,num=100) \n",
    "    k_min = 0\n",
    "    m_x_train, _, m_y_train, _ = train_test_split(x_train, y_train, test_size =1-data_ratio_orig[k_min], random_state = 10,\n",
    "                                     stratify=y_train)  \n",
    "    while (m_x_train[m_y_train>0.5,:].shape[0] <= n_splits) or (m_x_train[m_y_train<0.5,:].shape[0] <= n_splits):\n",
    "        k_min += 1\n",
    "        m_x_train, _, m_y_train, _ = train_test_split(x_train, y_train, test_size =1-data_ratio_orig[k_min], random_state = 10,\n",
    "                                 stratify=y_train)  \n",
    "    k_max = 99\n",
    "    m_x_train, _, m_y_train, _ = train_test_split(x_train, y_train, test_size =1-data_ratio_orig[k_max], random_state = 10,\n",
    "                                     stratify=y_train)  \n",
    "    while (m_x_train[m_y_train>0.5,:].shape[0] <= n_splits) or (m_x_train[m_y_train<0.5,:].shape[0] <= n_splits):\n",
    "        k_max -= 1\n",
    "        m_x_train, _, m_y_train, _ = train_test_split(x_train, y_train, test_size =1-data_ratio_orig[k_max], random_state = 10,\n",
    "                                 stratify=y_train)\n",
    "    return np.linspace(data_ratio_orig[k_min],data_ratio_orig[k_max],num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the learinig curve using k-fold cross validation. **Pay close attention to your inputs in** `train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C6\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, random_state=10, shuffle=True)\n",
    "data_ratio = max_data_ratio(n_splits,X_train, Y_train)\n",
    "\n",
    "J_train = np.zeros((2,len(data_ratio)))\n",
    "J_val = np.zeros((2,len(data_ratio)))\n",
    "\n",
    "pen = 'l2'\n",
    "solver = check_penalty(penalty=pen)\n",
    "lmb = best_lmbda\n",
    "lmb = 10\n",
    "for idx, curr_ratio in enumerate(data_ratio):\n",
    "    #--------------------------Impelment your code here:-------------------------------------\n",
    "\n",
    "    #----------------------------------------------------------------------------------------\n",
    "    with tqdm(total=n_splits, file=sys.stdout, position=0, leave=True) as pbar:\n",
    "        h = 0\n",
    "        J_train_fold = np.zeros(n_splits)\n",
    "        J_val_fold = np.zeros(n_splits)\n",
    "        m_x_train, _, m_y_train, _ = train_test_split(x_train_orig, y_train, test_size =1-curr_ratio, random_state = 10,\n",
    "                                             stratify=y_train)\n",
    "        for train_index, val_index in skf.split(m_x_train, m_y_train):\n",
    "            pbar.set_description('Dataset fraction is %.1f, processed folds' % curr_ratio)\n",
    "            pbar.update()\n",
    "            #--------------------------Impelment your code here:-------------------------------------\n",
    "         \n",
    "            #----------------------------------------------------------------------------------------\n",
    "            h += 1\n",
    "        #--------------------------Impelment your code here:-------------------------------------\n",
    "\n",
    "        #----------------------------------------------------------------------------------------\n",
    "fig, axes = plt.subplots(1,1,figsize=(10,10))\n",
    "axes.errorbar(np.ceil(data_ratio[5:]*x_train.shape[0]), J_train[0,5:], yerr=J_train[1,5:])\n",
    "axes.errorbar(np.ceil(data_ratio[5:]*x_train.shape[0]), J_val[0,5:], yerr=J_val[1,5:])\n",
    "axes.set_xlabel('# of examples in training set')\n",
    "axes.set_ylabel('Loss')\n",
    "axes.legend(['J_train', 'J_val (n = ' + str(x_val.shape[0]) + ')'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "<center><img src=\"outputs/6.PNG\" width=\"380\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, after we chose our hyperparameter(s), we can test our model performances! You should train your chosen model over the **whole training set** and report the performance upon the testing set. Don't forget to scale as needed. Name your scaled training and testing sets as `x_tr` and `x_tst`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "log_reg = LogisticRegression(random_state=5, penalty=pen, C = 1/best_lmbda, max_iter=max_iter,solver=solver)\n",
    "x_tr = scaler.fit_transform(X_train)\n",
    "x_tst = scaler.transform(x_test)\n",
    "log_reg.fit(x_tr, Y_train)\n",
    "y_pred_test = log_reg.predict(x_tst)\n",
    "y_pred_proba_test = log_reg.predict_proba(x_tst)\n",
    "plot_confusion_matrix(log_reg, x_tst, y_test, cmap=plt.cm.Blues)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "calc_TN = lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[0, 0]\n",
    "calc_FP = lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[0, 1]\n",
    "calc_FN = lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[1, 0]\n",
    "calc_TP = lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the performances mentioned above (except AUROC for now):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C7\n",
    "from sklearn.metrics import plot_confusion_matrix, roc_auc_score\n",
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "\n",
    "#-----------------------------------------------------------------------------------------\n",
    "print('AUROC is {:.2f}'.format(roc_auc_score(y_test, y_pred_proba_test[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "<center><img src=\"outputs/7.PNG\" width=\"380\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the performances are really bad. It is just slightly better than flipping a coin. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Question:***</span> *What are the possible reasons for that?*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "solver = 'liblinear'\n",
    "log_reg = LogisticRegression(random_state=5, max_iter=max_iter,solver=solver)\n",
    "lmbda = np.array([0.01, 0.01, 1, 10, 100, 1000])\n",
    "pipe = Pipeline(steps=[('scale', StandardScaler()), ('logistic', log_reg)])\n",
    "clf = GridSearchCV(estimator=pipe, param_grid={'logistic__C': 1/lmbda, 'logistic__penalty': ['l2']},\n",
    "                   scoring=['accuracy','f1','precision','recall','roc_auc'], cv=skf,\n",
    "                   refit='roc_auc', verbose=3, return_train_score=True)\n",
    "clf.fit(X_train, Y_train)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_radar(clf,params,clf_type):\n",
    "    labels=np.array(['Accuracy', 'F1', 'PPV', 'Sensitivity', 'AUROC']) \n",
    "    score_mat_train = np.stack((clf.cv_results_['mean_train_accuracy'], clf.cv_results_['mean_train_f1'],\n",
    "                               clf.cv_results_['mean_train_precision'], clf.cv_results_['mean_train_recall'],\n",
    "                               clf.cv_results_['mean_train_roc_auc']), axis=0)\n",
    "    score_mat_val = np.stack((clf.cv_results_['mean_test_accuracy'], clf.cv_results_['mean_test_f1'],\n",
    "                               clf.cv_results_['mean_test_precision'], clf.cv_results_['mean_test_recall'],\n",
    "                               clf.cv_results_['mean_test_roc_auc']), axis=0)\n",
    "\n",
    "\n",
    "    angles=np.linspace(0, 2*np.pi, len(labels), endpoint=False)\n",
    "    # close the plot\n",
    "\n",
    "    angles=np.concatenate((angles,[angles[0]]))\n",
    "    cv_dict = clf.cv_results_['params']\n",
    "    # cv_dict = sorted(cv_dict, key=lambda k: k['logistic__penalty'])  # arrange dictionaries by penalties\n",
    "    fig=plt.figure(figsize=(18,14))\n",
    "    for idx, loc in enumerate(cv_dict):\n",
    "        ax = fig.add_subplot(1, len(lmbda), 1+idx, polar=True)\n",
    "        stats_train = score_mat_train[:, idx]\n",
    "        stats_train=np.concatenate((stats_train,[stats_train[0]]))\n",
    "        ax.plot(angles, stats_train, 'o-', linewidth=2)\n",
    "        ax.fill(angles, stats_train, alpha=0.25)\n",
    "        stats_val = score_mat_val[:, idx]\n",
    "        stats_val=np.concatenate((stats_val,[stats_val[0]]))\n",
    "        ax.plot(angles, stats_val, 'o-', linewidth=2)\n",
    "        ax.fill(angles, stats_val, alpha=0.25)\n",
    "        ax.set_thetagrids(angles[0:-1] * 180/np.pi, labels) #angles[0:-1]\n",
    "        if idx == 0:\n",
    "            ax.set_ylabel('$L_2$', fontsize=18)\n",
    "        if cv_dict[idx]['logistic__C'] <= 1:\n",
    "            ax.set_title('$\\lambda$ = %d'  % (1 / cv_dict[idx]['logistic__C']))\n",
    "        else:\n",
    "            ax.set_title('$\\lambda$ = %.3f' % (1 / cv_dict[idx]['logistic__C']))\n",
    "        ax.set_ylim([0,1])\n",
    "        ax.legend(['Train','Validation'])\n",
    "        ax.grid(True)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will plot a *radar plot* in order to visualize the performances. Repeat C7 and in addtion plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#C8\n",
    "params = ['C', 'penalty']\n",
    "clf_type = 'log_reg'\n",
    "plot_radar(clf,params,clf_type)\n",
    "chosen_clf = clf.best_estimator_\n",
    "# print(chosen_clf)\n",
    "y_pred_test = chosen_clf.predict(x_test) #NOTICE NOT TO USE THE STANDARDIZED DATA.\n",
    "y_pred_proba_test = chosen_clf.predict_proba(x_test)\n",
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "print('AUROC is {:.2f}'.format(roc_auc_score(y_test, y_pred_proba_test[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "<center><img src=\"outputs/8.PNG\" width=\"550\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Question:***</span> *Try to rerun `GridSearchCV` using a different penalty. Did you get any different results?*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we would like to save the best model ($l_2$ regularization) in order to compare its' performances with other classifiers in the next tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(\"results\" in os.listdir()):\n",
    "    os.mkdir(\"results\")\n",
    "with open('results/best_lin.pkl', 'wb') as f: \n",
    "    pickle.dump(chosen_clf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images credit:\n",
    "* [Classification regularization](https://medium.com/ml-research-lab/under-fitting-over-fitting-and-its-solution-dc6191e34250)\n",
    "* [Regression regularization](https://www.quora.com/What-is-regularization-in-machine-learning)\n",
    "* Geometrical interpretation: From Bishop s Pattern recognition and machine learning, Figure 3.4.\n",
    "* [Diabetic-Retinopathy](https://2020vision4nh.org/diabetic-retinopathy/)\n",
    "* [Real diabetic retina](https://www.reviewofoptometry.com/article/my-patient-has-diabetic-retinopathynow-what)\n",
    "* [Fitting](http://madrury.github.io/smoothers/)\n",
    "* [Kfolds](http://ethen8181.github.io/machine-learning/model_selection/model_selection.html)\n",
    "* [Summary](http://www.cs.nthu.edu.tw/~shwu/courses/ml/labs/08_CV_Ensembling/08_CV_Ensembling.html)\n",
    "* [AUROC](https://www.researchgate.net/figure/Receiver-Operating-Characteristic-ROC-curves-and-the-area-under-ROC-curve-or-AUC_fig3_331797273)\n",
    "\n",
    "\n",
    "#### *This tutorial was written by [Moran Davoodi](mailto:morandavoodi@gmail.com) with the assitance of [Yuval Ben Sason](mailto:yuvalbse@gmail.com) & Kevin Kotzen*"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
